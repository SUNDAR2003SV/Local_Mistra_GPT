# Local Mistral GPT Model

This project runs a **Mistral 7B** model locally using **Ollama** with a **Flask backend** and a **HTML/CSS/JS frontend**.  
It allows you to chat with your local GPT model without any cloud dependency.

---

## üöÄ Features
- Runs **entirely on your local machine** ‚Äî no internet required for model responses.
- Uses **Ollama** to manage and serve the Mistral model.
- **Flask backend** for API endpoints.
- **HTML/CSS/JavaScript frontend** for user-friendly interaction.
- CORS enabled for cross-origin API calls.

---

## üõ† Tech Stack
- **Model**: Mistral 7B (via Ollama)
- **Backend**: Python 3, Flask, Flask-CORS
- **Frontend**: HTML, CSS, JavaScript
- **Platform**: Localhost

---

## üìÇ Project Structure
Local_GPT/
  static/
     style.css
  templates/
     index.html
app.py

‚öôÔ∏è Installation & Setup
1Ô∏è‚É£ Install Ollama
Download and install Ollama from the official site: https://ollama.ai
(Ollama is the platform that runs the AI models locally on your machine.)

2Ô∏è‚É£ Pull the Mistral Model
Once Ollama is installed, you can use different models.

We‚Äôll use Mistral 7B, a free and popular model. (You can choose another model if you prefer.)

üîπ Note: If you search "Mistral 7B" on the Ollama website, you‚Äôll find details about it, but you can‚Äôt download it directly from there ‚Äî you need to pull it via the command line.

Open your Command Prompt / Terminal and run:

ollama pull mistral
This will download the Mistral 7B model to your local Ollama setup so the backend can use it.

